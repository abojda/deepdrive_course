{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5DHHYu1UQIY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download and install deepdrive_course repository when running in Google Colab (to have access to the libraries)**"
      ],
      "metadata": {
        "id": "ywzS0HYCCJR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "  !git clone https://github.com/abojda/deepdrive_course.git dd_course\n",
        "  !pip install dd_course/ -q"
      ],
      "metadata": {
        "id": "5KC6zGcGAPS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "tZUsfbGgk9LO"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmrDr1Bg93Y"
      },
      "source": [
        "## wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31_obIkog9S2"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "TDqZm8kj6DNK"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "cEfWZmPDGpd-"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import STL10\n",
        "\n",
        "config = dict(\n",
        "  project_name = 'stl10_supervised',\n",
        "\n",
        "  run_name = 'scratch-onecycle_lr0.001-randaug_3',\n",
        "  # run_name = 'simclr_tl-onecycle_lr0.001-randaug_3',\n",
        "\n",
        "  image_size = 96,\n",
        "  input_dim = 2048, # Resnet50 features have 2048 dimensions\n",
        "\n",
        "  timm_model = 'resnet50',\n",
        "  timm_dropout = 0.3,\n",
        "\n",
        "  checkpoint = None,\n",
        "  # checkpoint = 'https://mega.nz/file/47E1gBQD#nlwXN6ygtYUuH6K9RFFgGu7i6pmDagtfE3TTxb7wmJw', # SimCLR checkpoint\n",
        "\n",
        "  epochs = 100,\n",
        "  batch_size = 64,\n",
        "  lr = 1e-3,\n",
        "  seed=42,\n",
        "\n",
        "  optimizer = 'Adam',\n",
        "  # optimizer = 'RMSprop',\n",
        "  optimizer_kwargs = {},\n",
        ")\n",
        "\n",
        "scheduler_config = dict(\n",
        "  # scheduler = None,\n",
        "  # scheduler_interval = 'step',\n",
        "  # scheduler_kwargs = {}\n",
        "\n",
        "  scheduler = 'OneCycleLR',\n",
        "  scheduler_interval = 'step',\n",
        "  scheduler_kwargs = dict(\n",
        "      epochs = config[\"epochs\"],\n",
        "      max_lr = config[\"lr\"],\n",
        "      # steps_per_epoch is updated after training DataLoader instantiation\n",
        "  ),\n",
        ")\n",
        "\n",
        "config.update(**scheduler_config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AogJQnXHAfFc"
      },
      "source": [
        "# Prepare data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZN8CVeZPAH4h"
      },
      "source": [
        "## Define data transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "cp2uwQuHAJ3K"
      },
      "outputs": [],
      "source": [
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, RandAugment\n",
        "\n",
        "train_transform = Compose([\n",
        "    Resize((config['image_size'], config['image_size'])),\n",
        "    RandAugment(num_ops=3),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "test_transform = Compose([\n",
        "    Resize((config['image_size'], config['image_size'])),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAjLUpcm6bOO"
      },
      "source": [
        "## Initialize datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3LDfBEsq9T6J"
      },
      "outputs": [],
      "source": [
        "from torchvision.datasets import STL10\n",
        "\n",
        "root = 'stl10_data'\n",
        "\n",
        "train_ds = STL10(root=root,\n",
        "                 split='train',\n",
        "                 transform = train_transform,\n",
        "                 download=True)\n",
        "\n",
        "test_ds = STL10(root=root,\n",
        "                split='test',\n",
        "                transform = test_transform,\n",
        "                download=True)\n",
        "\n",
        "# Update config\n",
        "config[\"classes\"] = train_ds.classes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDmruMyyx35I"
      },
      "source": [
        "## Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS1nd4EQx35L"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "seed_everything(config['seed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ETMnWZ3Fx35N"
      },
      "source": [
        "## Initialize dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mWRbbZzx35P"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import multiprocessing\n",
        "\n",
        "train_dl = DataLoader(train_ds,\n",
        "                      batch_size=config['batch_size'],\n",
        "                      shuffle=True,\n",
        "                      drop_last=False,\n",
        "                      num_workers=multiprocessing.cpu_count(),\n",
        "                      pin_memory=True,\n",
        "                      )\n",
        "\n",
        "test_dl = DataLoader(test_ds,\n",
        "                     batch_size=config['batch_size'],\n",
        "                     shuffle=False,\n",
        "                     drop_last=False,\n",
        "                     num_workers=multiprocessing.cpu_count(),\n",
        "                     pin_memory=True,\n",
        "                     )\n",
        "\n",
        "\n",
        "# Update steps_per_epoch in configuration dictionary\n",
        "config[\"scheduler_kwargs\"][\"steps_per_epoch\"] = int(len(train_dl) * config[\"limit_train_batches\"])\n",
        "print(config[\"scheduler_kwargs\"][\"steps_per_epoch\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4DkEgr8-JSs"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ABvPg0PefGdT"
      },
      "source": [
        "## Prepare backbone model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "TJou8KeHfItI"
      },
      "outputs": [],
      "source": [
        "from deepdrive_course.stl10.modules import LitSimCLR, LitClassifier\n",
        "from deepdrive_course.utils import download_from_mega_nz\n",
        "\n",
        "def get_model(config):\n",
        "  if config[\"checkpoint\"] is None:\n",
        "    # We don't use pretrained model. STL10 dataset contains images from Imagenet, so that would be cheating!\n",
        "    backbone = timm.create_model(\n",
        "      config['timm_model'],\n",
        "      num_classes=0,\n",
        "      pretrained=False,\n",
        "      drop_rate=config['timm_dropout'])\n",
        "\n",
        "  elif config['checkpoint'].endswith('.ckpt'):\n",
        "    simclr_model = LitSimCLR.load_from_checkpoint(config['checkpoint'])\n",
        "    backbone = simclr_model.backbone\n",
        "\n",
        "  elif 'mega.nz' in config['checkpoint']:\n",
        "    checkpoint = download_from_mega_nz(config['checkpoint'])\n",
        "    simclr_model = LitSimCLR.load_from_checkpoint(checkpoint)\n",
        "    backbone = simclr_model.backbone\n",
        "\n",
        "  else:\n",
        "    raise ValueError(config['checkpoint'])\n",
        "\n",
        "  return LitClassifier(backbone, config)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_model(config)"
      ],
      "metadata": {
        "id": "0BF596uth-LI"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q94Xme3od6Sx"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define callbacks"
      ],
      "metadata": {
        "id": "I9fKtORmXTL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(monitor='val_loss',\n",
        "                                save_top_k=3,\n",
        "                                dirpath=f'{config[\"project_name\"]}/best/{config[\"run_name\"]}',\n",
        "                                filename='{epoch}-{val_loss:.2f}')\n",
        "\n",
        "lr_monitor_cb = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "callbacks = [\n",
        "    checkpoint_cb,\n",
        "    lr_monitor_cb,\n",
        "]"
      ],
      "metadata": {
        "id": "nAjM9ATAXUik"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HFFkDHGH4sI8"
      },
      "source": [
        "## Training and validation loops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AqrXLa5R4sJA"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# Define logger\n",
        "logger = WandbLogger(project=config['project_name'], name=config['run_name'])\n",
        "logger.experiment.config.update(config)\n",
        "\n",
        "# Setup summary metrics\n",
        "logger.experiment.define_metric(\"val_loss\", summary=\"min\")\n",
        "logger.experiment.define_metric(\"val_acc\", summary=\"max\")\n",
        "logger.experiment.define_metric(\"train_loss\", summary=\"min\")\n",
        "logger.experiment.define_metric(\"train_acc\", summary=\"max\")\n",
        "\n",
        "try:\n",
        "  trainer = pl.Trainer(\n",
        "      max_epochs=config['epochs'],\n",
        "      logger=logger,\n",
        "      callbacks=callbacks,\n",
        "      num_sanity_val_steps=0,\n",
        "  )\n",
        "\n",
        "  trainer.fit(model, train_dl, test_dl)\n",
        "finally:\n",
        "  wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}