{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L5DHHYu1UQIY"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ywzS0HYCCJR3"
   },
   "source": [
    "**Download and install deepdrive_course repository when running in Google Colab (to have access to the libraries)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5KC6zGcGAPS5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "in_colab = \"google.colab\" in sys.modules\n",
    "\n",
    "if in_colab:\n",
    "    !git clone https://github.com/abojda/deepdrive_course.git dd_course\n",
    "    !pip install dd_course/ -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install fiftyone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ELmrDr1Bg93Y"
   },
   "source": [
    "## wandb login"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "31_obIkog9S2"
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "5_7ZjCoa7_ay"
   },
   "outputs": [],
   "source": [
    "import timm\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bq7F3ebX0bM1"
   },
   "source": [
    "# Prepare data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G5lV4s2x0d8C"
   },
   "source": [
    "## Download images with FiftyOne\n",
    "Warning: This may take a while and requires ~5GB of disk space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zpNYbiTknMnI"
   },
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "\n",
    "classes = [\"Bathtub\", \"Sink\"]\n",
    "splits = [\"train\", \"validation\"]\n",
    "\n",
    "fo_dataset = fo.zoo.load_zoo_dataset(\n",
    "    \"open-images-v7\",\n",
    "    dataset_dir=\"fo_raw_data\",\n",
    "    splits=splits,\n",
    "    label_types=[\"classifications\"],\n",
    "    classes=classes,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HbeMFkdA0j-p"
   },
   "source": [
    "## Export images to `data/split/classname` directories\n",
    "Original splits contain:\n",
    "- train:\n",
    "  - 829 Bathtub images\n",
    "  - 3359 Sink images\n",
    "- validation:\n",
    "  - 17 Bathtub images\n",
    "  - 49 Sink images\n",
    "\n",
    "Original validation split is only ~1.5% of all available images for these classes.\n",
    "\n",
    "Therefore we will combine all images into one dataset and later perform 80/20 split on our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "odYFwuD7nnWf",
    "outputId": "10359c2c-c020-45db-ddc2-58dae84536e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Bathtub images: 846\n",
      " 100% |█████████████████| 846/846 [1.0s elapsed, 0s remaining, 868.0 samples/s]         \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |█████████████████| 846/846 [1.0s elapsed, 0s remaining, 868.0 samples/s]         \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sink images: 3408\n",
      " 100% |███████████████| 3408/3408 [4.3s elapsed, 0s remaining, 819.2 samples/s]      \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:eta.core.utils: 100% |███████████████| 3408/3408 [4.3s elapsed, 0s remaining, 819.2 samples/s]      \n"
     ]
    }
   ],
   "source": [
    "for _class in classes:\n",
    "    view = fo_dataset.filter_labels(\"positive_labels\", fo.ViewField(\"label\") == _class)\n",
    "    print(f\"\\n{_class} images: {len(view)}\")\n",
    "\n",
    "    view.export(\n",
    "        export_dir=f\"data/{_class.lower()}\", dataset_type=fo.types.ImageDirectory\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QoGItBlj06C4"
   },
   "source": [
    "## Load PyTorch datasets from these directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "wvakFjHnzWv6"
   },
   "outputs": [],
   "source": [
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, RandAugment\n",
    "from deepdrive_course.utils import stratified_train_test_split\n",
    "\n",
    "\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        Resize((224, 224)),\n",
    "        RandAugment(num_ops=3),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        Resize((224, 224)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_ds = ImageFolder(root=\"data\")\n",
    "\n",
    "train_ds, val_ds, _, _ = stratified_train_test_split(\n",
    "    full_ds,\n",
    "    train_size=0.8,\n",
    "    train_transform=train_transform,\n",
    "    test_transform=val_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DaIW-6fH6l2s"
   },
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "oS-Kmr1HBBHj"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "pos_samples = np.count_nonzero(full_ds.targets)\n",
    "neg_samples = len(full_ds.targets) - pos_samples\n",
    "\n",
    "config = dict(\n",
    "    project_name=\"binary_bathtub_sink\",\n",
    "    # run_name=\"baseline-onecyle_lr0.0001\",\n",
    "    run_name=\"baseline-onecyle_lr0.0001-weighted_loss\",\n",
    "    \n",
    "    classes=full_ds.classes,\n",
    "\n",
    "    timm_model=\"resnet50\",\n",
    "    timm_pretrained=True,\n",
    "    timm_dropout=0.3,\n",
    "\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    lr=1e-4,\n",
    "    seed=42,\n",
    "\n",
    "    # pos_class_weight = None,\n",
    "    pos_class_weight=neg_samples / pos_samples,\n",
    "\n",
    "    optimizer=\"Adam\",\n",
    "    # optimizer=\"RMSprop\",\n",
    "    optimizer_kwargs={},\n",
    ")\n",
    "\n",
    "\n",
    "scheduler_config = dict(\n",
    "    # scheduler = None,\n",
    "    # scheduler_interval=\"step\",\n",
    "    # scheduler_kwargs = {}\n",
    "\n",
    "    scheduler=\"OneCycleLR\",\n",
    "    scheduler_interval=\"step\",\n",
    "    scheduler_kwargs=dict(\n",
    "        epochs=config[\"epochs\"],\n",
    "        max_lr=config[\"lr\"],\n",
    "        # steps_per_epoch is updated after training DataLoader instantiation\n",
    "    ),\n",
    ")\n",
    "\n",
    "config.update(**scheduler_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zv4e3ZacBBrO"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qr_kgnNPLeFD"
   },
   "outputs": [],
   "source": [
    "from deepdrive_course.openimages_binary.modules import LitBinaryClassifier\n",
    "\n",
    "model = timm.create_model(\n",
    "    config[\"timm_model\"],\n",
    "    num_classes=1,\n",
    "    pretrained=config[\"timm_pretrained\"],\n",
    "    drop_rate=config[\"timm_dropout\"],\n",
    ")\n",
    "\n",
    "model = LitBinaryClassifier(model, config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q94Xme3od6Sx"
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DEYmlmVUeHn0"
   },
   "source": [
    "## Reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xadK0gA6eKN9"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "\n",
    "seed_everything(config[\"seed\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ugNjIAuiIAX3"
   },
   "source": [
    "## Initialize dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6DxILCY_ICfj"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "import multiprocessing\n",
    "\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=config[\"batch_size\"],\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    "    num_workers=multiprocessing.cpu_count(),\n",
    "    pin_memory=True,\n",
    ")\n",
    "\n",
    "\n",
    "# Update steps_per_epoch in configuration dictionary\n",
    "config[\"scheduler_kwargs\"][\"steps_per_epoch\"] = len(train_dl)\n",
    "print(config[\"scheduler_kwargs\"][\"steps_per_epoch\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9fKtORmXTL3"
   },
   "source": [
    "## Define callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "nAjM9ATAXUik"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateMonitor,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "\n",
    "checkpoint_cb = ModelCheckpoint(\n",
    "    monitor=\"val_loss\",\n",
    "    save_top_k=3,\n",
    "    dirpath=f'{config[\"project_name\"]}/best/{config[\"run_name\"]}',\n",
    "    filename=\"{epoch}-{val_loss:.2f}\",\n",
    ")\n",
    "\n",
    "lr_monitor_cb = LearningRateMonitor(logging_interval=\"step\")\n",
    "\n",
    "callbacks = [\n",
    "    checkpoint_cb,\n",
    "    lr_monitor_cb,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FtMTjLaye41p"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# Define logger\n",
    "logger = WandbLogger(project=config[\"project_name\"], name=config[\"run_name\"])\n",
    "logger.experiment.config.update(config)\n",
    "\n",
    "# Setup summary metrics\n",
    "logger.experiment.define_metric(\"val_loss\", summary=\"min\")\n",
    "logger.experiment.define_metric(\"val_accuracy\", summary=\"max\")\n",
    "logger.experiment.define_metric(\"val_balanced_accuracy\", summary=\"max\")\n",
    "logger.experiment.define_metric(\"train_loss\", summary=\"min\")\n",
    "logger.experiment.define_metric(\"train_accuracy\", summary=\"max\")\n",
    "logger.experiment.define_metric(\"train_balanced_accuracy\", summary=\"max\")\n",
    "\n",
    "\n",
    "try:\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=config[\"epochs\"],\n",
    "        logger=logger,\n",
    "        callbacks=callbacks,\n",
    "        num_sanity_val_steps=0,\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dl, val_dl)\n",
    "finally:\n",
    "    wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
