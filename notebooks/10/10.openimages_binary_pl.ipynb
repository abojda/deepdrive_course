{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5DHHYu1UQIY"
      },
      "source": [
        "# Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Download and install deepdrive_course repository when running in Google Colab (to have access to the libraries)**"
      ],
      "metadata": {
        "id": "ywzS0HYCCJR3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "in_colab = 'google.colab' in sys.modules\n",
        "\n",
        "if in_colab:\n",
        "  !git clone https://github.com/abojda/deepdrive_course.git dd_course\n",
        "  !pip install dd_course/ -q"
      ],
      "metadata": {
        "id": "5KC6zGcGAPS5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ELmrDr1Bg93Y"
      },
      "source": [
        "## wandb login"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "31_obIkog9S2"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import timm\n",
        "import pytorch_lightning as pl"
      ],
      "metadata": {
        "id": "5_7ZjCoa7_ay"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare data"
      ],
      "metadata": {
        "id": "bq7F3ebX0bM1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download images with FiftyOne\n",
        "Warning: This may take a while and requires ~5GB of disk space"
      ],
      "metadata": {
        "id": "G5lV4s2x0d8C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import fiftyone as fo\n",
        "\n",
        "classes=[\"Bathtub\", \"Sink\"]\n",
        "splits = [\"train\", \"validation\"]\n",
        "\n",
        "fo_dataset = fo.zoo.load_zoo_dataset(\n",
        "              \"open-images-v7\",\n",
        "              dataset_dir=\"fo_raw_data\",\n",
        "              splits=splits,\n",
        "              label_types=[\"classifications\"],\n",
        "              classes=classes,\n",
        "          )"
      ],
      "metadata": {
        "id": "zpNYbiTknMnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Export images to `data/split/classname` directories\n",
        "Original splits contain:\n",
        "- train:\n",
        "  - 829 Bathtub images\n",
        "  - 3359 Sink images\n",
        "- validation:\n",
        "  - 17 Bathtub images\n",
        "  - 49 Sink images\n",
        "\n",
        "Original validation split is only ~1.5% of all available images for these classes.\n",
        "\n",
        "Therefore we will combine all images into one dataset and later perform 80/20 split on our own."
      ],
      "metadata": {
        "id": "HbeMFkdA0j-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for _class in classes:\n",
        "  view = fo_dataset.filter_labels(\"positive_labels\", fo.ViewField(\"label\") == _class)\n",
        "  print(f'\\n{_class} images: {len(view)}')\n",
        "\n",
        "  view.export(\n",
        "      export_dir=f'data/{_class.lower()}',\n",
        "      dataset_type=fo.types.ImageDirectory\n",
        "      )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "odYFwuD7nnWf",
        "outputId": "10359c2c-c020-45db-ddc2-58dae84536e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Bathtub images: 846\n",
            " 100% |█████████████████| 846/846 [1.0s elapsed, 0s remaining, 868.0 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |█████████████████| 846/846 [1.0s elapsed, 0s remaining, 868.0 samples/s]         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sink images: 3408\n",
            " 100% |███████████████| 3408/3408 [4.3s elapsed, 0s remaining, 819.2 samples/s]      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:eta.core.utils: 100% |███████████████| 3408/3408 [4.3s elapsed, 0s remaining, 819.2 samples/s]      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load PyTorch datasets from these directories"
      ],
      "metadata": {
        "id": "QoGItBlj06C4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import Compose, ToTensor, Normalize, Resize, RandAugment\n",
        "from deepdrive_course.utils import stratified_train_test_split\n",
        "\n",
        "\n",
        "train_transform = Compose([\n",
        "    Resize((224,224)),\n",
        "    RandAugment(num_ops=3),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "val_transform = Compose([\n",
        "    Resize((224,224)),\n",
        "    ToTensor(),\n",
        "    Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "full_ds = ImageFolder(root='data')\n",
        "\n",
        "train_ds, val_ds, _, _ = stratified_train_test_split(full_ds,\n",
        "                                                     train_size=0.8,\n",
        "                                                     train_transform=train_transform,\n",
        "                                                     test_transform=val_transform)"
      ],
      "metadata": {
        "id": "wvakFjHnzWv6"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Config"
      ],
      "metadata": {
        "id": "DaIW-6fH6l2s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "pos_samples = np.count_nonzero(full_ds.targets)\n",
        "neg_samples = len(full_ds.targets) - pos_samples\n",
        "\n",
        "config = dict(\n",
        "  project_name = 'binary_bathtub_sink',\n",
        "  # run_name = 'baseline-onecyle_lr0.0001',\n",
        "  run_name = 'baseline-onecyle_lr0.0001-weighted_loss',\n",
        "\n",
        "  classes = full_ds.classes,\n",
        "\n",
        "  timm_model = 'resnet50',\n",
        "  timm_pretrained = True,\n",
        "  timm_dropout = 0.3,\n",
        "\n",
        "  epochs = 30,\n",
        "  batch_size = 64,\n",
        "  lr = 1e-4,\n",
        "  seed = 42,\n",
        "\n",
        "  # pos_class_weight = None,\n",
        "  pos_class_weight = neg_samples / pos_samples,\n",
        "\n",
        "  optimizer = 'Adam',\n",
        "  # optimizer = 'RMSprop',\n",
        "  optimizer_kwargs = {},\n",
        ")\n",
        "\n",
        "\n",
        "scheduler_config = dict(\n",
        "  # scheduler = None,\n",
        "  # scheduler_interval = 'step',\n",
        "  # scheduler_kwargs = {}\n",
        "\n",
        "  scheduler = 'OneCycleLR',\n",
        "  scheduler_interval = 'step',\n",
        "  scheduler_kwargs = dict(\n",
        "      epochs = config[\"epochs\"],\n",
        "      max_lr = config[\"lr\"],\n",
        "      # steps_per_epoch is updated after training DataLoader instantiation\n",
        "  ),\n",
        ")\n",
        "\n",
        "config.update(**scheduler_config)"
      ],
      "metadata": {
        "id": "oS-Kmr1HBBHj"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model"
      ],
      "metadata": {
        "id": "Zv4e3ZacBBrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from deepdrive_course.openimages_binary.modules import LitBinaryClassifier\n",
        "\n",
        "model = timm.create_model(\n",
        "    config['timm_model'],\n",
        "    num_classes=1,\n",
        "    pretrained=config['timm_pretrained'],\n",
        "    drop_rate=config['timm_dropout']\n",
        ")\n",
        "\n",
        "model = LitBinaryClassifier(model, config)"
      ],
      "metadata": {
        "id": "qr_kgnNPLeFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q94Xme3od6Sx"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEYmlmVUeHn0"
      },
      "source": [
        "## Reproducibility"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xadK0gA6eKN9"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning import seed_everything\n",
        "\n",
        "seed_everything(config['seed'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ugNjIAuiIAX3"
      },
      "source": [
        "## Initialize dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6DxILCY_ICfj"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "import multiprocessing\n",
        "\n",
        "train_dl = DataLoader(train_ds,\n",
        "                      batch_size=config['batch_size'],\n",
        "                      shuffle=True,\n",
        "                      drop_last=True,\n",
        "                      num_workers=multiprocessing.cpu_count(),\n",
        "                      pin_memory=True,\n",
        "                      )\n",
        "\n",
        "val_dl = DataLoader(val_ds,\n",
        "                    batch_size=config['batch_size'],\n",
        "                    shuffle=False,\n",
        "                    drop_last=True,\n",
        "                    num_workers=multiprocessing.cpu_count(),\n",
        "                    pin_memory=True,\n",
        "                    )\n",
        "\n",
        "\n",
        "# Update steps_per_epoch in configuration dictionary\n",
        "config[\"scheduler_kwargs\"][\"steps_per_epoch\"] = len(train_dl)\n",
        "print(config[\"scheduler_kwargs\"][\"steps_per_epoch\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define callbacks"
      ],
      "metadata": {
        "id": "I9fKtORmXTL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pytorch_lightning.callbacks import EarlyStopping, LearningRateMonitor, ModelCheckpoint\n",
        "\n",
        "checkpoint_cb = ModelCheckpoint(monitor='val_loss',\n",
        "                                save_top_k=3,\n",
        "                                dirpath=f'{config[\"project_name\"]}/best/{config[\"run_name\"]}',\n",
        "                                filename='{epoch}-{val_loss:.2f}')\n",
        "\n",
        "lr_monitor_cb = LearningRateMonitor(logging_interval='step')\n",
        "\n",
        "callbacks = [\n",
        "    checkpoint_cb,\n",
        "    lr_monitor_cb,\n",
        "]"
      ],
      "metadata": {
        "id": "nAjM9ATAXUik"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FtMTjLaye41p"
      },
      "outputs": [],
      "source": [
        "from pytorch_lightning.loggers import WandbLogger\n",
        "\n",
        "# Define logger\n",
        "logger = WandbLogger(project=config['project_name'], name=config['run_name'])\n",
        "logger.experiment.config.update(config)\n",
        "\n",
        "# Setup summary metrics\n",
        "logger.experiment.define_metric(\"val_loss\", summary=\"min\")\n",
        "logger.experiment.define_metric(\"val_accuracy\", summary=\"max\")\n",
        "logger.experiment.define_metric(\"val_balanced_accuracy\", summary=\"max\")\n",
        "logger.experiment.define_metric(\"train_loss\", summary=\"min\")\n",
        "logger.experiment.define_metric(\"train_accuracy\", summary=\"max\")\n",
        "logger.experiment.define_metric(\"train_balanced_accuracy\", summary=\"max\")\n",
        "\n",
        "\n",
        "try:\n",
        "  trainer = pl.Trainer(\n",
        "      max_epochs=config['epochs'],\n",
        "      logger=logger,\n",
        "      callbacks=callbacks,\n",
        "      num_sanity_val_steps=0,\n",
        "  )\n",
        "\n",
        "  trainer.fit(model, train_dl, val_dl)\n",
        "finally:\n",
        "  wandb.finish()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}